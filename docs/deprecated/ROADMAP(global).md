ðŸ“‰ Gap Analysis: PRD vs. Current Reality1. Audio Engine (The Ears)FeaturePRD RequirementCurrent StateThe GapSource SeparationReal Demucs v4 isolation.Mock Mode (returns file as-is).The system cannot actually isolate vocals yet.AnalysisDetect Onsets, BPM, and Stress.Onsets/BPM work (w/ 5% error). Stress is hardcoded false.The "robot" flow problem cannot be solved without stress detection.InputClean Mono 16kHz stem.Raw upload only.No normalization/preprocessing pipeline.2. Generation Engine (The Brain)FeaturePRD RequirementCurrent StateThe GapAI ModelGroq/GPT-4 via API.Mock Class (Hardcoded strings).No actual text generation capability exists.ValidationPhonetic count + Stress match.Phonetic count works. Stress match missing.The engine validates syllable counts but ignores rhythm/accents.Logic"Generate Many, Filter Best".Logic exists but tests dummy data.Ready for real LLM connection.3. Editor Interface (The Hands)FeaturePRD RequirementCurrent StateThe GapCorrectionTap-to-Rhythm (Spacebar).Read-only playback.User cannot fix bad automatic detection.EditingSplit, Merge, Delete regions.Resize/Drag only.User is stuck with the initial segmentation.FeedbackVisual Lyrics on timeline.None.No way to see the generated result on the grid.ðŸ—ºï¸ The "Road to 1.0" RoadmapTo move from "Prototype" to "Product," you need to execute these three milestones in order.ðŸš© Milestone 1: The "Real" Backend (Logic Completion)Goal: Replace all mock data with real DSP and AI processing.Implement Amplitude Stress Detection (audio_engine.py)Task: Analyze the audio envelope within each segment.Logic: If a peak amplitude within a segment is > 70% of the local max, mark is_stressed = true.Why: Essential for the LLM to generate rhythmic flow (DA-da-DA vs da-da-DA).Integrate Real LLM (phase0_blind_test.py -> main.py)Task: Replace the LyricGenerator mock with a client for Groq (Llama 3) or OpenAI.Prompt: Inject the syllable counts and stress boolean arrays into the system prompt.Activate DemucsTask: Enable the real Demucs separator in audio_engine.py.Optimization: Ensure it runs asynchronously so it doesn't block the API (consider a background worker if deploying, or keep it simple/slow for MVP).ðŸš© Milestone 2: The "Real" Editor (Frontend Completion)Goal: Give the user full control to fix the "Yaourt" segmentation.Build "Tap-to-Rhythm" ModeTask: Create a mode where the user listens to the track and presses Space.Code: Record timestamps of keypresses -> Clear old regions -> Generate new regions based on keypress intervals.Why: Automatic detection fails on mumbled audio. This is your "Safety Net."Implement Region Actions (Split/Merge)Task: Add context menu or keyboard shortcuts to:Split: Divide one region into two (cursor position).Merge: Join selected region with the next one.Delete: Remove artifacts/breaths.Snap-to-GridTask: Force region edges to snap to the nearest 1/16th or 1/8th note based on the detected BPM.ðŸš© Milestone 3: End-to-End IntegrationGoal: Connect the Editor to the Generator.The Generation EndpointTask: Create POST /generate in FastAPI.Input: Receives the validated JSON from the frontend (after user edits).Process: Runs the "Generate Many, Filter Best" logic using the user's grid.Streaming Response (SSE)Task: Stream the generated lines back to the UI line-by-line so the user doesn't wait 30 seconds for the whole song.Lyrics VisualizationTask: Render the text inside the Wavesurfer regions or in a synchronized karaoke view below the waveform.