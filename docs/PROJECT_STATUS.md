# ðŸ“Š Flow-to-Lyrics: Project Status Report

**Last Updated**: 2025-12-28  
**Version**: MVP - English Only (Post Phase D - Full-Audio Whisper)  
**Objective**: Transform informal vocal flows ("yaourt") into coherent rap/song lyrics with strict rhythmic precision and human validation.

---

## ðŸ“‹ Executive Summary

The Flow-to-Lyrics project is currently at approximately **75% completion** of the MVP roadmap. The **backend pipeline is COMPLETE** with all 5 core engines working end-to-end:

1. âœ… **AudioEngine** - Analyzes audio, detects segments with stress/sustain/pitch
2. âœ… **PromptEngine** - Translates PivotJSON to LLM prompts with melodic guidance
3. âœ… **GenerationEngine** - Generates candidates via Ollama (local or cloud)
4. âœ… **LyricValidator** - The "Gatekeeper" that filters by syllable count and groove score
5. âœ… **CorePipeline** - End-to-end orchestrator with multi-candidate exposure
6. âœ… **WhisperPhoneticAnalyzer** - Whisper + g2p_en for phonetic transcription (Phase C)
7. âœ… **Full-Audio Syllable Alignment** - Word-level timestamps with syllable distribution (Phase D)

The frontend remains a **read-only audio viewer** with no editing or generation capabilities exposed.

### Current User Experience
Users can only:
1. **Import** an audio file (drag-and-drop or file picker)
2. **View** the waveform with auto-detected segments
3. **Play** the audio with spacebar control
4. **View** segment details in synchronized data table

**No editing, no lyric generation UI, no export.** Lyric generation works via CLI only.

| Phase | Status | User-Facing? |
|-------|--------|--------------|
| Phase 0: Blind Test (Lyric Validation) | âœ… Complete | âŒ CLI only |
| Phase 1: Precision Engine | âœ… Complete | âŒ Backend only |
| Phase 2: End-to-End Integration | âš ï¸ 75% Complete | âŒ Not exposed |
| Phase 3: Co-Pilot UI | ðŸ”´ Not Started | ðŸ”´ Missing |

---

## ðŸ”„ PRD Pipeline vs. Current Implementation

### Ã‰tape 1: Nettoyage & Isolation (Audio Pre-processing)

| Feature | PRD Requirement | Current State | Status |
|---------|-----------------|---------------|--------|
| Input formats | WAV/MP3 | MP3, WAV, M4A, FLAC, OGG âœ… | âœ… Done |
| Demucs v4 (Hybrid Transformer) | Required | Implemented with mock mode | âš ï¸ Partial |
| Vocal isolation | Separate vocals vs instrumental | Code exists, defaults to `MOCK_MODE=true` | âš ï¸ Partial |
| Mono 16kHz conversion | Required for optimal analysis | **Not implemented** | ðŸ”´ Missing |
| Normalized vocal stem | Required | **Not implemented** | ðŸ”´ Missing |

**Files Involved**:
- `audio_engine.py` â†’ `DemucsProcessor` class (lines 95-179)

---

### Ã‰tape 2: Extraction Structurelle & Validation UX

| Feature | PRD Requirement | Current State | Status |
|---------|-----------------|---------------|--------|
| Onset detection (Spectral Flux) | Librosa | âœ… `librosa.onset.onset_detect()` with adaptive params | âœ… Done |
| **Adaptive onset detection** | Required | âœ… Spectral + energy-based fallback | âœ… Done |
| **Segment auto-splitting** | Required | âœ… Long segments split at energy valleys | âœ… Done |
| **Breath filtering** | Required | âœ… Low-energy segments filtered | âœ… Done |
| Intensity/Stress detection | Amplitude peaks | âœ… RMS amplitude analysis | âœ… Done |
| Sustain detection | Duration threshold | âœ… Duration-based detection | âœ… Done |
| **Pitch detection** | Required | âœ… `librosa.pyin` pitch tracking | âœ… Done |
| Interactive waveform | Wavesurfer.js + Regions | âœ… Fully functional | âœ… Done |
| Region drag/resize | Required | âš ï¸ Visual only, not persisted | âš ï¸ Partial |
| Merge/Split actions | Required | **Not implemented** | ðŸ”´ Missing |
| Delete regions | Required | **Not implemented** | ðŸ”´ Missing |
| Tap-to-Rhythm (Space key) | Manual marker placement | **Not implemented** | ðŸ”´ Missing |

**Files Involved**:
- `audio_engine.py` â†’ `LibrosaAnalyzer`, `PivotFormatter` classes
- `frontend/components/AudioEditor.tsx` (527 lines)
- `frontend/components/SegmentList.tsx` (164 lines)

---

### Ã‰tape 3: Le JSON Pivot

| Feature | PRD Requirement | Current State | Status |
|---------|-----------------|---------------|--------|
| `meta.tempo` | Required | âœ… Implemented | âœ… Done |
| `meta.duration` | Required | âœ… Implemented | âœ… Done |
| `meta.genre` | Required | **Not implemented** | ðŸ”´ Missing |
| `meta.theme` | Required | **Not implemented** | ðŸ”´ Missing |
| `meta.language` | Required (en-US) | **Not implemented** | ðŸ”´ Missing |
| `blocks[].id` | Required | âœ… Implemented | âœ… Done |
| `blocks[].rhyme_scheme` | Required | **Not implemented** | ðŸ”´ Missing |
| `blocks[].syllable_target` | Required | âœ… Auto-calculated | âœ… Done |
| `segments[].time_start` | Required | âœ… Implemented | âœ… Done |
| `segments[].duration` | Required | âœ… Implemented | âœ… Done |
| `segments[].is_stressed` | Required | âœ… Dynamic RMS detection | âœ… Done |
| `segments[].is_sustained` | Required | âœ… Duration threshold | âœ… Done |
| `segments[].pitch_contour` | Required | âœ… **NEW** - pyin detection | âœ… Done |

**Current Output Structure**:
```json
{
  "meta": { "tempo": 123.05, "duration": 11.65 },
  "blocks": [{
    "id": 1,
    "syllable_target": 5,
    "segments": [
      { 
        "time_start": 0.07, 
        "duration": 0.186, 
        "is_stressed": true,
        "is_sustained": false,
        "pitch_contour": "mid"
      }
    ]
  }],
  "_meta": { "filename": "test.mp3", "mock_mode": true, "llm_model": "ministral-3:8b" }
}
```

---

### Ã‰tape 4: GÃ©nÃ©ration & Validation PhonÃ©tique

| Feature | PRD Requirement | Current State | Status |
|---------|-----------------|---------------|--------|
| "Generate Many, Filter Best" strategy | Required | âœ… 5-candidate generation | âœ… Done |
| g2p_en phonetic validation | Required | âœ… Fully implemented | âœ… Done |
| Syllable counting (auditory) | Required | âœ… Works correctly | âœ… Done |
| Stress pattern matching | Required | âœ… `LyricValidator.calculate_groove_score()` | âœ… Done |
| **Weighted Groove Scoring** | Required | âœ… 2x weight for stressed beats | âœ… Done |
| LLM integration (Local Ollama) | Required | âœ… `GenerationEngine` | âœ… Done |
| **Cloud Ollama support** | Optional | âœ… API key authentication | âœ… Done |
| Parallel 5-candidate generation | Required | âœ… Full pipeline | âœ… Done |
| Syllabic scoring (0 or 1) | Required | âœ… `LyricValidator.validate_line()` | âœ… Done |
| Stress scoring (0.0 - 1.0) | Required | âœ… Groove Score | âœ… Done |
| Retry with error-specific prompts | Required | **Not implemented** | ðŸ”´ Missing |
| **Prompt Engine** | Required | âœ… External templates | âœ… Done |
| **Pitch/Melodic Guidance** | Required | âœ… **NEW** - Injected in prompts | âœ… Done |
| **Core Pipeline** | Required | âœ… `CorePipeline` orchestrator | âœ… Done |
| **Multi-Candidate Exposure** | Required | âœ… `GenerationResult` returns all 5 | âœ… Done |

**Files Involved**:
- `validator.py` â†’ `LyricValidator` class
- `core_pipeline.py` â†’ `CorePipeline`, `GenerationResult` classes
- `generation_engine.py` â†’ `GenerationEngine` class
- `prompt_engine.py` â†’ `PromptEngine` class
- `prompts/system_instruction.md` â†’ System prompt
- `prompts/user_template.md` â†’ User template with `{{pitch_guidance}}`

**Test Results** (Precision Tuning - 2025-12-28):

| Test File | Expected Syllables | Detected | Error |
|-----------|-------------------|----------|-------|
| 3_syllabes(sustained)_test.mp3 | 3 | 3 | âœ“ 0 |
| 3_syllabes_test.mp3 | 3 | 3 | âœ“ 0 |
| 5_syllabes_test.mp3 | 5 | 5 | âœ“ 0 |
| 10_syllabes_test.mp3 | 10 | 11 | +1 |
| **test_audio_2-1.m4a** | **6** | **6** | **âœ“ 0** |

---

### Ã‰tape 5: Alignement & Rendu

| Feature | PRD Requirement | Current State | Status |
|---------|-----------------|---------------|--------|
| CTC-Segmentation | Optional | **Not implemented** | ðŸ”´ Missing |
| Linear alignment | Alternative | **Not implemented** | ðŸ”´ Missing |
| Word-by-word text display | Required | **Not implemented** | ðŸ”´ Missing |
| Audio-text synchronization | Required | **Not implemented** | ðŸ”´ Missing |
| SSE streaming | Required | **Not implemented** | ðŸ”´ Missing |
| Export functionality | Required | **Not implemented** | ðŸ”´ Missing |

---

## ðŸ› ï¸ Technology Stack Comparison

### Backend (Python)

| Technology | PRD | Current | Status |
|------------|-----|---------|--------|
| FastAPI (Async, Websockets) | Required | âœ… FastAPI implemented | âš ï¸ No Websockets |
| Librosa | Required | âœ… Installed & used | âœ… Done |
| Demucs | Required | âœ… Installed (mock mode) | âš ï¸ Partial |
| g2p_en | Required | âœ… Fully functional | âœ… Done |
| Instructor/Outlines | Required for JSON | Robust regex parsing | âš ï¸ Alternative |
| Local Ollama | Required | âœ… Fully integrated | âœ… Done |
| Cloud Ollama | Optional | âœ… API key authentication | âœ… Done |
| **Config Module** | Implied | âœ… Centralized `.env` loading | âœ… Done |

### Frontend (Next.js / React)

| Technology | PRD | Current | Status |
|------------|-----|---------|--------|
| Next.js 14 | Implied | âœ… Installed | âœ… Done |
| Wavesurfer.js v7 | Required | âœ… Fully integrated | âœ… Done |
| Regions Plugin | Required | âœ… Working | âœ… Done |
| Timeline Plugin | Required | **Not implemented** | ðŸ”´ Missing |
| Zustand | Required | âœ… Fully integrated | âœ… Done |
| SSE (Server-Sent Events) | Required | **Not implemented** | ðŸ”´ Missing |

---

## âœ… What The User Can Actually Do (Current UX)

Based on the current frontend interface, users can perform **4 core actions only**:

| Action | Works? | Notes |
|--------|--------|-------|
| ðŸŽµ **Import audio file** | âœ… | Drag-and-drop or click (MP3, WAV, M4A, FLAC, OGG) |
| ðŸ‘ï¸ **View waveform + segments** | âœ… | See detected syllable regions overlaid on waveform |
| â–¶ï¸ **Play/pause audio** | âœ… | Button or Spacebar shortcut |
| ðŸ“Š **View segment table** | âœ… | Bi-directional sync with waveform hover/active |

**That's it.** Everything else is backend-only or not exposed to the user.

### What Appears to Work But Doesn't

| Feature | Visual State | Reality |
|---------|--------------|---------|
| Drag/resize regions | Regions appear draggable | Changes aren't saved or exported |
| Zoom controls | Slider exists | Works but has no practical use |
| BPM display | Shows value | Informational only |

### Backend Infrastructure (Working but Not User-Facing)
1. **FastAPI server** (`main.py`) - Runs on `localhost:8000`
2. **Audio upload endpoint** (`POST /upload`) - Returns PivotJSON
3. **Full lyric generation pipeline** (`CorePipeline`) - Works via CLI
4. **5-candidate LLM generation** (`GenerationEngine`) - Returns all options
5. **Phonetic validation** (`LyricValidator`) - g2p_en groove scoring
6. **Pitch detection** (`PivotFormatter`) - librosa.pyin integration
7. **Prompt Engine** (`prompt_engine.py`) - Melodic guidance injection

---
## ðŸŽ¯ Target Frontend Experience (Missing vs Current)
The current frontend is a **temporary testing prototype** with poor UX ("trash" tier). The final implementation requires a complete overhaul to support true interactivity.
### 1. Interactivity Requirements (Currently Missing)
The user **IS NOT** currently able to effectively edit the segmentation. The target experience requires:
- [ ] **Drag & Resize**: Users must be able to freely move and resize segment regions.
- [ ] **Split & Merge**: Ability to cut a segment in two or join two segments.
- [ ] **Delete & Add**: Intuitive controls to remove false positives or add missing syllables.
- [ ] **Snap-to-Grid**: Segments should optionally snap to rhythm quantization.
### 2. Visual Requirements (Currently Basic)
The current waveform overlay is merely functional. The target design needs:
- [ ] **Distinct Blocks**: Regions should look like solid, interactive blocks that are **superposed directly onto the waveform** for easy edits.
- [ ] **Clear Handles**: Visual cues for resizing (left/right handles).
- [ ] **Hover Effects**: Clear visual feedback when hovering over editable zones.
- [ ] **Context Menus**: Right-click actions for specific segment operations.
> **Status**: The current UI exists purely to validate the backend data flow. A dedicated UI/UX phase is pending to build the actual editor.
---
## ðŸ”´ What Needs Refinement
### Critical (Blocks Core Functionality)
1. **Real LLM integration** - Currently mock only
3. **Tap-to-Rhythm feature** - Manual marker placement missing
4. **Region Split/Merge/Delete** - Editing actions missing
5. **End-to-end pipeline** - No connection between audio analysis and lyric generation
### Important (Affects Quality)
1. **BPM accuracy** - ~5% variance needs improvement
3. **Mono 16kHz conversion** - Missing audio preprocessing
4. **Stress pattern matching** - No scoring implemented
5. **Pitch contour detection** - Missing from Pivot JSON
### Nice to Have
1. **Timeline plugin** - Visual time markers
2. **SSE streaming** - Real-time lyric display
3. **Export functionality** - Save edited segments
4. **Genre/Theme metadata** - Not captured
5. **Multi-block support** - Only `blocks[0]` is rendered

---

## âš™ï¸ Syllable Detection Configuration

As of 2025-12-28, onset detection parameters are configurable via `.env`:

| Variable | Default | Description |
|----------|---------|-------------|
| `ONSET_DELTA` | `0.05` | Detection sensitivity (lower = more sensitive) |
| `ONSET_USE_ENERGY` | `true` | Enable energy-based fallback detection |
| `MAX_SEGMENT_DURATION` | `1.0` | Max segment length before auto-splitting |
| `ONSET_WAIT` | `1` | Min frames between onsets |

## âš™ï¸ Phonetic Analysis Configuration (Phase C)

As of 2025-12-28, phonetic analysis uses Whisper + g2p_en with Allosaurus fallback:

| Variable | Default | Description |
|----------|---------|-------------|
| `PHONETIC_MODEL` | `whisper` | Backend: `whisper` (recommended) or `allosaurus` |
| `WHISPER_MODEL_SIZE` | `base` | Model size: `tiny`, `base`, `small`, `medium`, `large` |
| `PHONETIC_ENABLED` | `true` | Enable IPA phoneme extraction |
| `PHONETIC_MIN_DURATION` | `0.10` | Min segment duration for analysis (100ms) |
| `PHONETIC_PADDING` | `0.05` | Context padding on each side (50ms) |
| `PHONETIC_RETRY_PADDING` | `0.10` | Expanded retry padding on failure (100ms) |
| `PHONETIC_FALLBACK_ENABLED` | `true` | Return `[vowel]`/`[consonant]` when detection fails |

> â„¹ï¸ **Phase C Complete:** Whisper + g2p_en pipeline improves accuracy for mumbled vocals. Detection rate 60-83% depending on audio quality. See [REMAINING_ISSUES.md](./REMAINING_ISSUES.md) for known limitations.

---

## ðŸ“… Roadmap Progress

### Phase 0: "Blind Test" (Weeks 1-2) â†’ âœ… 100% Complete
- [x] Python script with syllable input
- [x] g2p_en phonetic validation
- [x] "Generate Many, Filter Best" logic
- [x] Prompt Engine (JSON-to-Prompt translation)
- [x] Real LLM integration (Ollama)
- [x] Validator with Groove Score (0.0-1.0)
- [x] Core Pipeline orchestrating all engines

### Phase 1: Precision Engine â†’ âœ… 100% Complete
- [x] Pitch detection (`librosa.pyin`)
- [x] Pitch contour mapping (low/mid/high/rising/falling)
- [x] Multi-candidate exposure (`GenerationResult`)
- [x] Groove score calibration (2x weight for stressed beats)
- [x] Melodic guidance in prompts (`{{pitch_guidance}}`)
- [x] Precision tuning script (`test_precision_tuning.py`)
- [x] Onset detection optimization (delta=0.1)

### Phase 2: Segmentation Tool â†’ âš ï¸ 75% Complete
- [x] Wavesurfer.js frontend
- [x] Demucs backend (mock mode)
- [x] Audio â†’ Pivot JSON pipeline
- [x] Region visualization
- [x] Stress & Sustain detection
- [x] Config module (`.env` support)
- [x] Cloud Ollama support
- [ ] Tap-to-Rhythm feature
- [ ] Region editing (split/merge/delete)

### Phase 3: End-to-End Integration â†’ âš ï¸ 50% Complete
- [x] Connect Phase 0 + Phase 1 (via `core_pipeline.py`)
- [x] Full pipeline testing (all tests passing)
- [ ] API endpoint for lyrics generation (`POST /generate/interactive`)
- [ ] SSE streaming for lyrics
- [ ] Export functionality

### Phase 4: Co-Pilot UI â†’ ðŸ”´ 0% Complete
- [ ] Candidate List UI component ("Slot Machine")
- [ ] Click-to-Apply lyric selection
- [ ] "Regenerate" button
- [ ] Region locking
- [ ] Context menu for segment actions

### Phonetic Improvement: Phase A & B â†’ âœ… 100% Complete
- [x] Add segment padding (50ms context on each side)
- [x] Increase min_duration (100ms minimum)
- [x] Add retry with expanded padding (100ms on failure)
- [x] Add `[vowel]`/`[consonant]` fallback classification
- [x] Add 5 new config options (`PHONETIC_*`)
- [x] Create `classify_sound_type()` spectral fallback
- [x] Tests: 8 new tests in `test_phonetic_padding.py`

> **Result:** Detection rate improved from 67% â†’ 83%, but **accuracy issue remains** (see `docs/PHONETIC_ACCURACY_ISSUE.md`).

### Phonetic Improvement: Phase C â†’ âœ… 100% Complete
- [x] Create `WhisperPhoneticAnalyzer` class
- [x] Integrate Whisper for transcription
- [x] Convert words â†’ phonemes via g2p_en
- [x] Add `PHONETIC_MODEL` config option (`allosaurus`/`whisper`)
- [x] Add `WHISPER_MODEL_SIZE` config option
- [x] Automatic fallback to Allosaurus when Whisper unavailable
- [x] Tests: 7 new tests in `test_whisper_phonetic.py`

> **Result:** Whisper integration provides context-aware transcription for mumbled vocals, with g2p_en converting words to accurate English phonemes.

---

## ðŸ“ Project Structure

```
Lyrics.ai/
â”œâ”€â”€ main.py                     # FastAPI server (223 lines)
â”œâ”€â”€ audio_engine.py             # DSP: Demucs + Librosa + Pitch (631 lines)
â”œâ”€â”€ prompt_engine.py            # JSON-to-Prompt translation (351 lines)
â”œâ”€â”€ generation_engine.py        # Ollama LLM integration (421 lines)
â”œâ”€â”€ validator.py                # LyricValidator - The Gatekeeper (365 lines)
â”œâ”€â”€ core_pipeline.py            # CorePipeline - The Orchestrator (422 lines)
â”œâ”€â”€ config.py                   # Centralized config with .env (211 lines)
â”œâ”€â”€ phase0_blind_test.py        # Original validation script (362 lines)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env / .env.example         # Configuration
â”œâ”€â”€ test_audio_real.mp3         # Test audio files
â”œâ”€â”€ prompts/                    # LLM prompt templates
â”‚   â”œâ”€â”€ system_instruction.md   # Persona + few-shot examples
â”‚   â””â”€â”€ user_template.md        # User prompt with pitch guidance
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_audio_analysis.py  # Stress/sustain tests
â”‚   â”œâ”€â”€ test_prompt_engine.py   # Prompt generation tests
â”‚   â”œâ”€â”€ test_generation.py      # LLM integration tests
â”‚   â”œâ”€â”€ test_end_to_end.py      # Full pipeline tests
â”‚   â””â”€â”€ test_precision_tuning.py # Onset calibration
â”œâ”€â”€ audio samples/              # Precision tuning audio files
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ PROJECT_STATUS.md       # This file
â”‚   â”œâ”€â”€ NEXT_PHASES.md
â”‚   â”œâ”€â”€ PHASE1_PRECISION_CHANGELOG.md
â”‚   â””â”€â”€ prd.md
â””â”€â”€ frontend/
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ page.tsx            # Main page layout
    â”‚   â”œâ”€â”€ layout.tsx          # Root layout
    â”‚   â””â”€â”€ globals.css         # Dark theme styles
    â”œâ”€â”€ components/
    â”‚   â”œâ”€â”€ AudioEditor.tsx     # Waveform editor (527 lines)
    â”‚   â””â”€â”€ SegmentList.tsx     # Segment table (164 lines)
    â”œâ”€â”€ store/
    â”‚   â””â”€â”€ useAudioStore.ts    # Zustand state (138 lines)
    â””â”€â”€ lib/
        â””â”€â”€ api.ts              # Backend API client
```

---

## ðŸš€ Recommended Next Steps

### Immediate (This Week)
1. **Create `/generate/interactive` API endpoint**
   - Accept `region_id` and `context` parameters
   - Return all 5 candidates with scores
   - Connect frontend to this endpoint

2. **Build Candidate List UI Component**
   - Display 5 lyric options with scores
   - Click-to-apply functionality
   - "Regenerate" button

### Short-term (Next 2 Weeks)
3. **Implement Tap-to-Rhythm** in `AudioEditor.tsx`
4. **Add Region Split/Merge/Delete actions**
5. **File Slicing** for audio > 4 seconds

### Medium-term (Weeks 3-4)
6. **SSE streaming** for real-time lyric display
7. **Export functionality** (JSON, SRT/VTT)
8. **Region locking** for approved lyrics

---

## ðŸ§ª Known Issues

| Issue | Location | Severity | Notes |
|-------|----------|----------|-------|
| BPM variance ~5% | `LibrosaAnalyzer.analyze()` | Medium | May need prior estimation |
| Mock Demucs only | `DemucsProcessor` | Medium | Real processing needs GPU |
| Single block rendering | `AudioEditor.tsx` | Low | Only `blocks[0]` displayed |
| 10-syllable file +1 error | `test_precision_tuning.py` | Low | Edge case in onset detection |
| Frontend not connected to generation | `frontend/` | High | No lyric generation UI |

---

*This document was last updated by analyzing the project codebase on 2025-12-28.*
