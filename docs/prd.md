# üìÑ Document de Sp√©cifications Techniques : Projet "Flow-to-Lyrics" (v2.1)

**Scope** : MVP (Anglais uniquement)  
**Objectif** : Transformer un flux vocal informel ("yaourt") en paroles de rap/chanson coh√©rentes, avec une pr√©cision rythmique stricte et une validation humaine.

---

## 1. üîÑ Le Pipeline de Traitement (Workflow "Human-in-the-Loop")

Ce pipeline privil√©gie la pr√©cision rythmique sur la vitesse pure, en combinant la puissance des LLM avec une validation logique rigide (Neuro-Symbolic AI).

### √âtape 1 : Nettoyage & Isolation (Audio Pre-processing)
*   **Input** : Fichier audio brut (WAV/MP3).
*   **Technologie** : Demucs v4 (Hybrid Transformer).
*   **Processus** :
    *   S√©paration des sources (Vocals vs Instrumental).
    *   Normalisation du stem vocal.
    *   Conversion en Mono 16kHz (format optimal pour l'analyse spectrale).
*   **Output** : `vocal_stem.wav`.

### √âtape 2 : Extraction Structurelle & Validation UX (Le "Safety Check")
C'est ici que se joue la qualit√© finale. On ne fait pas confiance aveugl√©ment √† l'algo.

*   **Analyse Automatique** :
    *   *D√©tection d'Onsets* : Librosa (Spectral Flux) pour rep√©rer les d√©buts de syllabes.
    *   *D√©tection d'Intensit√©* : Rep√©rage des pics d'amplitude pour deviner les accents toniques (Stress).
*   **üõë Interface "Human-in-the-loop" (Frontend)** :
    *   *Visuel* : Waveform interactive (via Wavesurfer.js + Region Plugin). Chaque r√©gion = 1 syllabe.
    *   *Feature "Tap-to-Rhythm"* : Si la d√©tection automatique √©choue (trop de bruit), l'utilisateur peut r√©√©couter le son et appuyer sur une touche (Espace) en rythme pour red√©finir les marqueurs de syllabes manuellement.
    *   *Actions* : Merge (fusionner), Split (couper), Delete.
*   **Output** : Une "Grid" valid√©e de segments temporels.

### √âtape 3 : Le JSON Pivot (Enrichi pour l'Anglais)
Structure de donn√©es envoy√©e au backend de g√©n√©ration.

```json
{
  "meta": {
    "tempo": 90,
    "genre": "Trap",
    "theme": "Overcoming obstacles",
    "language": "en-US"
  },
  "blocks": [
    {
      "id": 1,
      "rhyme_scheme": "A",
      "syllable_target": 8, 
      "segments": [
        { 
          "time_start": 0.0, 
          "duration": 0.2, 
          "is_stressed": true,  // Important pour l'anglais (Strong beat)
          "pitch_contour": "high" // Pour sugg√©rer une voyelle ouverte
        },
        // ... suite des segments
      ]
    }
  ]
}
```

### √âtape 4 : G√©n√©ration & Validation Phon√©tique (Moteur Hybride)
Remplacement de la boucle it√©rative simple par une g√©n√©ration parall√®le + filtrage.

*   **Strat√©gie** : "Generate Many, Filter Best"
    *   Au lieu de demander 1 ligne et de la corriger, on demande au LLM de g√©n√©rer 5 variantes d'une m√™me ligne en parall√®le.
*   **Le Prompt (System Prompt)** :
    *   Injection de contraintes structurelles : *"Write a line of exactly 8 syllables. Stress pattern should roughly match: DA-da-DA-da..."*
*   **Le Validateur (Python - The "Gatekeeper")** :
    *   *Technologie* : `g2p_en` (Grapheme-to-Phoneme) ou CMU Dict.
    *   *Logique* : Convertir le texte en phon√®mes (ex: "Fire" -> F AY1 ER0). Compter les noyaux vocaliques pour obtenir le vrai compte syllabique auditif, et non orthographique.
*   **Scoring** :
    *   *Score Syllabique (0 ou 1)* : Le compte est-il exact ?
    *   *Score de Stress (0.0 - 1.0)* : Les mots accentu√©s tombent-ils sur les segments `is_stressed` ?
*   **S√©lection** : On garde la meilleure variante. Si aucune ne matche, on relance un batch avec un prompt d'erreur sp√©cifique.

### √âtape 5 : Alignement & Rendu
*   **Technologie** : CTC-Segmentation (si possible) ou alignement lin√©aire simple bas√© sur les timestamps valid√©s √† l'√©tape 2.
*   **Output** : Texte affich√© mot par mot sur l'interface, synchronis√© avec l'audio original.

---

## 2. üõ†Ô∏è Stack Technique (Mise √† Jour)

### Backend (Python)
*   **Core** : FastAPI (Async, Websockets).
*   **Audio Processing** : Torchaudio, Librosa, Demucs.
*   **NLP / Phon√©tique (Anglais)** :
    *   `g2p_en` : Pour la conversion texte -> phon√®mes (tr√®s pr√©cis pour l'anglais).
    *   `nltk` (CMU Dict) : Base de donn√©es lexicale.
*   **LLM Integration** :
    *   Instructor ou Outlines : Pour forcer une sortie JSON valide (Structured Generation).
    *   *Mod√®les* : Groq (Llama-3-70b) pour la vitesse (Drafting) ou GPT-4o (si complexit√© s√©mantique √©lev√©e).

### Frontend (Next.js / React)
*   **Audio UI** : Wavesurfer.js (v7) + Plugins (Regions, Timeline).
*   **State Management** : Zustand (pour g√©rer l'√©tat complexe de l'√©diteur audio).
*   **Communication** : Server-Sent Events (SSE) pour voir les lignes appara√Ætre en temps r√©el.

---

## 3. üö¶ Analyse des Risques (Mise √† jour v2.1)

| Risque Critique | Solution Technique |
| :--- | :--- |
| **Syllabation Anglaise** (Ex: "Every" = 2 syllabes, pas 3) | Utilisation de G2P (Phon√®mes). Ne jamais utiliser de compteurs bas√©s sur l'orthographe (pyphen) pour le rap. On compte les sons, pas les lettres. |
| **Latence** (L'utilisateur attend trop) | G√©n√©ration Parall√®le (Batching). G√©n√©rer 5 candidats en un appel API est aussi rapide qu'en g√©n√©rer 1. Le filtrage Python est instantan√© (ms). |
| **Erreur de segmentation** (Le "Yaourt" est illisible) | Feature "Tap-to-Rhythm". Permettre √† l'utilisateur de taper le rythme au clavier pour corriger l'IA instantan√©ment. |
| **Flow "Robotique"** | D√©tection d'accents (Stress). Mapper les temps forts de l'audio aux syllabes accentu√©es du texte via CMU Dict (1 = Primary Stress). |

---

## 4. üìÖ Roadmap R√©vis√©e (Focus MVP Anglais)

### Phase 0 : Le "Blind Test" (Semaines 1-2) - Priorit√© Absolue
*   **Objectif** : Valider le moteur de g√©n√©ration sans interface graphique.
*   **Action** : Script Python qui prend une liste `[8, 10, 8, 10]` syllabes.
*   **Test** : G√©n√©ration via LLM -> Validation via `g2p_en`.
*   **KPI** : Atteindre >90% de lignes valides rythmiquement.

### Phase 1 : L'Outil de Segmentation (Semaines 3-4)
*   D√©veloppement du Frontend Wavesurfer.js.
*   Impl√©mentation de Demucs (Backend).
*   Feature "Tap-to-Rhythm" fonctionnelle.
*   Pas encore de g√©n√©ration de texte, juste Audio -> Blocs JSON.

### Phase 2 : Int√©gration End-to-End (Semaines 5-6)
*   Connexion du moteur Phase 0 avec l'interface Phase 1.
*   Streaming des r√©ponses.
*   Export initial.